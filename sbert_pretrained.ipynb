{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sbert_pretrained.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Yu_Gii4iYhO",
        "outputId": "a3d94278-c778-4efb-d573-97ab912f1dd5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.7/dist-packages (1.7.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yhMF0BnKeOgc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "from transformers import FNetTokenizer, FNetModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/df_with_label.csv').sample(3000)"
      ],
      "metadata": {
        "id": "W4pzU_Pcefv3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tX7Y5JtmetpD",
        "outputId": "b85c817c-6d0f-49f6-9422-d02e8a97148d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              sentence_a  \\\n",
              "54680  In order to initiate development of a structur...   \n",
              "38812  We compared the set of genes correctly detecte...   \n",
              "10345  We have previously shown that two co-stimulato...   \n",
              "88475  Let us briefly review the Holm [13] method, wh...   \n",
              "5609   We searched these genomes for a Pfam motif, PF...   \n",
              "\n",
              "                                              sentence_b  label  \n",
              "54680  Some Acacia species are shade intolerant resul...      0  \n",
              "38812  PromoterInspector results were mapped to pseud...      1  \n",
              "10345  Transduced cells were stained for membrane exp...      1  \n",
              "88475  This finding is inconsistent with the recent s...      0  \n",
              "5609   An example being the enzyme glyceraldehyde-3-p...      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b89c1fd8-30c6-4067-b76b-6adc9ef0809a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_a</th>\n",
              "      <th>sentence_b</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>54680</th>\n",
              "      <td>In order to initiate development of a structur...</td>\n",
              "      <td>Some Acacia species are shade intolerant resul...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38812</th>\n",
              "      <td>We compared the set of genes correctly detecte...</td>\n",
              "      <td>PromoterInspector results were mapped to pseud...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10345</th>\n",
              "      <td>We have previously shown that two co-stimulato...</td>\n",
              "      <td>Transduced cells were stained for membrane exp...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88475</th>\n",
              "      <td>Let us briefly review the Holm [13] method, wh...</td>\n",
              "      <td>This finding is inconsistent with the recent s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5609</th>\n",
              "      <td>We searched these genomes for a Pfam motif, PF...</td>\n",
              "      <td>An example being the enzyme glyceraldehyde-3-p...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b89c1fd8-30c6-4067-b76b-6adc9ef0809a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b89c1fd8-30c6-4067-b76b-6adc9ef0809a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b89c1fd8-30c6-4067-b76b-6adc9ef0809a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, val = train_test_split(df, test_size=0.25)\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "fnet_tokenizer = FNetTokenizer.from_pretrained('google/fnet-base')"
      ],
      "metadata": {
        "id": "-l5pIfNeeAuq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NegativeSamplingDataset(Dataset):\n",
        "    \"\"\"\n",
        "    ToxicCommentsDataset is created to create a custom dataset.\n",
        "    later we wrap a lightning data module around it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data: pd.DataFrame, tokenizer: BertTokenizer, max_token_len: int):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_token_len = max_token_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        data_row = self.data.iloc[index]\n",
        "        sent1 = data_row['sentence_a']\n",
        "        sent2 = data_row['sentence_b']\n",
        "        label = data_row['label'].flatten()\n",
        "\n",
        "        encoding1 = self.tokenizer.encode_plus(\n",
        "\n",
        "            sent1,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "\n",
        "        )\n",
        "        encoding2 = self.tokenizer.encode_plus(\n",
        "\n",
        "            sent2,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_token_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "\n",
        "        )\n",
        "\n",
        "        return dict(\n",
        "            input_ids1=encoding1[\"input_ids\"].flatten(),\n",
        "            attention_mask1=encoding1[\"attention_mask\"].flatten(),\n",
        "            input_ids2=encoding2[\"input_ids\"].flatten(),\n",
        "            attention_mask2=encoding2[\"attention_mask\"].flatten(),\n",
        "            labels=torch.tensor(label, dtype=torch.long))"
      ],
      "metadata": {
        "id": "sI8_JsXecGDc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = NegativeSamplingDataset(train, bert_tokenizer, 64)\n",
        "val_dataset = NegativeSamplingDataset(val, bert_tokenizer, 64)\n",
        "\n",
        "bert_train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True,\n",
        "                              num_workers=1)\n",
        "\n",
        "bert_eval_dataloader = DataLoader(val_dataset, batch_size=16, num_workers=1)"
      ],
      "metadata": {
        "id": "knUssSiIcDVb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = NegativeSamplingDataset(train, fnet_tokenizer, 64)\n",
        "val_dataset = NegativeSamplingDataset(val, fnet_tokenizer, 64)\n",
        "\n",
        "fnet_train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True,\n",
        "                              num_workers=1)\n",
        "\n",
        "fnet_eval_dataloader = DataLoader(val_dataset, batch_size=16, num_workers=1)"
      ],
      "metadata": {
        "id": "0_UcZ-A5SV2E"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SBertModule(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super(SBertModule, self).__init__(config)\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, input_ids, segment_ids=None, input_mask=None):\n",
        "        outputs = self.bert(input_ids, segment_ids, input_mask)\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        \n",
        "        input_mask_expanded = input_mask.unsqueeze(-1).expand(sequence_output.size()).float()\n",
        "        sum_embeddings = torch.sum(sequence_output * input_mask_expanded, 1)\n",
        "        sum_mask = input_mask_expanded.sum(1)\n",
        "        sum_mask = torch.clamp(sum_mask, 1e-9)\n",
        "        mean_pooling_out = sum_embeddings / sum_mask  # [batch_size, hidden_size]\n",
        "        return mean_pooling_out\n",
        "            \n",
        "\n",
        "class SBert(nn.Module):\n",
        "    def __init__(self, model_path=None, config=None):\n",
        "        super(SBert, self).__init__()\n",
        "        self.num_labels = 2\n",
        "        self.bert_module = SBertModule.from_pretrained(model_path)\n",
        "        self.linear1 = nn.Linear(768 * 3, 768)\n",
        "        self.linear2 = nn.Linear(768, self.num_labels)\n",
        "\n",
        "    def forward(self, x_input_ids=None, x_segment_ids=None, x_input_mask=None,\n",
        "                      y_input_ids=None, y_segment_ids=None, y_input_mask=None, labels=None, train_sbert=True):\n",
        "      if train_sbert:\n",
        "        u = self.bert_module(x_input_ids, x_segment_ids, x_input_mask)\n",
        "        v = self.bert_module(y_input_ids, y_segment_ids, y_input_mask)\n",
        "        uv = torch.sub(u, v)\n",
        "        uv_abs = torch.abs(uv)\n",
        "        output = torch.cat([u, v, uv_abs], dim=-1)\n",
        "        \n",
        "        output = F.relu(self.linear1(output))\n",
        "        logits = self.linear2(output)\n",
        "        return logits\n",
        "\n",
        "      else:\n",
        "        return self.bert_module(x_input_ids, x_segment_ids, x_input_mask)"
      ],
      "metadata": {
        "id": "dGEGJh3peXqF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FnetModule(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super(FnetModule, self).__init__(config)\n",
        "        self.bert = FNetModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, input_ids, segment_ids=None, input_mask=None):\n",
        "        outputs = self.bert(input_ids, segment_ids, input_mask)\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        \n",
        "        input_mask_expanded = input_mask.unsqueeze(-1).expand(sequence_output.size()).float()\n",
        "        sum_embeddings = torch.sum(sequence_output * input_mask_expanded, 1)\n",
        "        sum_mask = input_mask_expanded.sum(1)\n",
        "        sum_mask = torch.clamp(sum_mask, 1e-9)\n",
        "        mean_pooling_out = sum_embeddings / sum_mask  # [batch_size, hidden_size]\n",
        "        return mean_pooling_out\n",
        "            \n",
        "\n",
        "class SFnet(nn.Module):\n",
        "    def __init__(self, model_path=None, config=None):\n",
        "        super(SFnet, self).__init__()\n",
        "        self.num_labels = 2\n",
        "        self.fnet_module = FnetModule.from_pretrained(model_path)\n",
        "        self.linear1 = nn.Linear(768 * 3, 768)\n",
        "        self.linear2 = nn.Linear(768, self.num_labels)\n",
        "\n",
        "    def forward(self, x_input_ids=None, x_segment_ids=None, x_input_mask=None,\n",
        "                      y_input_ids=None, y_segment_ids=None, y_input_mask=None, labels=None, train_sbert=True):\n",
        "      if train_sbert:\n",
        "        u = self.fnet_module(x_input_ids, x_segment_ids, x_input_mask)\n",
        "        v = self.fnet_module(y_input_ids, y_segment_ids, y_input_mask)\n",
        "        uv = torch.sub(u, v)\n",
        "        uv_abs = torch.abs(uv)\n",
        "        output = torch.cat([u, v, uv_abs], dim=-1)\n",
        "        \n",
        "        output = F.relu(self.linear1(output))\n",
        "        logits = self.linear2(output)\n",
        "        return logits\n",
        "\n",
        "      else:\n",
        "        return self.fnet_module(x_input_ids, x_segment_ids, x_input_mask)"
      ],
      "metadata": {
        "id": "rwoBCcgJP_BZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = SBert(model_path='bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dsd7L8geZLn",
        "outputId": "5551cc87-c898-43f7-aae5-63516eab1218"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing SBertModule: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing SBertModule from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing SBertModule from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fnet_model = SFnet(model_path='google/fnet-base')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g15YSS_yP78R",
        "outputId": "ef1d824c-ebb7-4f6c-d4de-4a589fae9060"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type fnet to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
            "You are using a model of type bert to instantiate a model of type fnet. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing FNetModel: ['bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias']\n",
            "- This IS expected if you are initializing FNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FNetModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.fourier.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.6.fourier.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.3.fourier.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.fourier.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.fourier.output.LayerNorm.weight', 'embeddings.projection.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.fourier.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.fourier.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.0.fourier.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.fourier.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.fourier.output.LayerNorm.weight', 'encoder.layer.5.fourier.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.2.fourier.output.LayerNorm.bias', 'encoder.layer.7.fourier.output.LayerNorm.weight', 'encoder.layer.4.fourier.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.fourier.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.fourier.output.LayerNorm.bias', 'encoder.layer.8.fourier.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.fourier.output.LayerNorm.bias', 'encoder.layer.10.fourier.output.LayerNorm.bias', 'encoder.layer.4.fourier.output.LayerNorm.weight', 'encoder.layer.6.fourier.output.LayerNorm.bias', 'encoder.layer.3.fourier.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.8.fourier.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'pooler.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'pooler.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.fourier.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'embeddings.projection.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at google/fnet-base were not used when initializing FnetModule: ['fnet.encoder.layer.5.output.LayerNorm.bias', 'fnet.encoder.layer.3.output.LayerNorm.bias', 'fnet.embeddings.LayerNorm.weight', 'fnet.encoder.layer.7.output.LayerNorm.bias', 'fnet.encoder.layer.10.output.LayerNorm.bias', 'fnet.encoder.layer.3.output.dense.bias', 'fnet.encoder.layer.8.intermediate.dense.bias', 'fnet.encoder.layer.6.fourier.output.LayerNorm.weight', 'fnet.encoder.layer.6.output.dense.weight', 'fnet.encoder.layer.4.fourier.output.LayerNorm.weight', 'fnet.encoder.layer.2.output.dense.weight', 'fnet.encoder.layer.7.intermediate.dense.weight', 'fnet.encoder.layer.5.output.dense.bias', 'fnet.embeddings.position_embeddings.weight', 'fnet.encoder.layer.5.intermediate.dense.weight', 'fnet.encoder.layer.9.fourier.output.LayerNorm.bias', 'fnet.embeddings.projection.weight', 'cls.predictions.transform.dense.bias', 'fnet.encoder.layer.2.output.dense.bias', 'fnet.encoder.layer.4.output.dense.bias', 'fnet.encoder.layer.5.fourier.output.LayerNorm.bias', 'fnet.encoder.layer.1.fourier.output.LayerNorm.bias', 'fnet.encoder.layer.0.intermediate.dense.weight', 'fnet.encoder.layer.9.output.dense.bias', 'fnet.encoder.layer.0.output.dense.weight', 'fnet.encoder.layer.6.output.LayerNorm.weight', 'fnet.encoder.layer.6.output.LayerNorm.bias', 'fnet.encoder.layer.5.output.LayerNorm.weight', 'fnet.encoder.layer.2.intermediate.dense.weight', 'fnet.encoder.layer.0.output.LayerNorm.bias', 'fnet.encoder.layer.3.fourier.output.LayerNorm.weight', 'fnet.encoder.layer.11.output.LayerNorm.weight', 'fnet.encoder.layer.11.fourier.output.LayerNorm.weight', 'cls.seq_relationship.bias', 'fnet.embeddings.projection.bias', 'fnet.encoder.layer.11.output.LayerNorm.bias', 'fnet.encoder.layer.0.intermediate.dense.bias', 'fnet.encoder.layer.7.output.LayerNorm.weight', 'fnet.encoder.layer.1.intermediate.dense.bias', 'fnet.encoder.layer.5.output.dense.weight', 'fnet.encoder.layer.6.intermediate.dense.weight', 'fnet.encoder.layer.0.fourier.output.LayerNorm.bias', 'fnet.encoder.layer.8.fourier.output.LayerNorm.weight', 'fnet.encoder.layer.6.output.dense.bias', 'fnet.encoder.layer.2.fourier.output.LayerNorm.weight', 'fnet.encoder.layer.10.intermediate.dense.bias', 'fnet.encoder.layer.3.output.LayerNorm.weight', 'fnet.encoder.layer.11.output.dense.bias', 'fnet.embeddings.token_type_embeddings.weight', 'fnet.encoder.layer.10.output.dense.weight', 'fnet.encoder.layer.8.fourier.output.LayerNorm.bias', 'fnet.encoder.layer.11.fourier.output.LayerNorm.bias', 'fnet.encoder.layer.7.output.dense.weight', 'fnet.encoder.layer.6.intermediate.dense.bias', 'fnet.encoder.layer.3.output.dense.weight', 'fnet.embeddings.position_ids', 'fnet.encoder.layer.4.intermediate.dense.bias', 'fnet.encoder.layer.2.intermediate.dense.bias', 'fnet.encoder.layer.10.fourier.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'fnet.encoder.layer.10.output.dense.bias', 'cls.predictions.transform.dense.weight', 'fnet.encoder.layer.1.intermediate.dense.weight', 'fnet.encoder.layer.1.fourier.output.LayerNorm.weight', 'fnet.encoder.layer.6.fourier.output.LayerNorm.bias', 'fnet.encoder.layer.4.output.dense.weight', 'fnet.embeddings.word_embeddings.weight', 'fnet.encoder.layer.8.output.dense.bias', 'fnet.encoder.layer.11.output.dense.weight', 'fnet.pooler.dense.bias', 'fnet.encoder.layer.4.output.LayerNorm.bias', 'fnet.encoder.layer.7.intermediate.dense.bias', 'fnet.encoder.layer.7.fourier.output.LayerNorm.weight', 'fnet.encoder.layer.9.intermediate.dense.weight', 'fnet.encoder.layer.9.intermediate.dense.bias', 'fnet.encoder.layer.3.intermediate.dense.bias', 'fnet.encoder.layer.1.output.dense.bias', 'fnet.encoder.layer.3.fourier.output.LayerNorm.bias', 'fnet.encoder.layer.1.output.dense.weight', 'fnet.encoder.layer.8.output.dense.weight', 'fnet.embeddings.LayerNorm.bias', 'fnet.encoder.layer.11.intermediate.dense.bias', 'fnet.encoder.layer.9.fourier.output.LayerNorm.weight', 'fnet.encoder.layer.5.fourier.output.LayerNorm.weight', 'fnet.encoder.layer.7.fourier.output.LayerNorm.bias', 'fnet.encoder.layer.8.intermediate.dense.weight', 'fnet.encoder.layer.4.fourier.output.LayerNorm.bias', 'cls.seq_relationship.weight', 'fnet.pooler.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'fnet.encoder.layer.3.intermediate.dense.weight', 'fnet.encoder.layer.10.fourier.output.LayerNorm.weight', 'cls.predictions.decoder.weight', 'fnet.encoder.layer.9.output.LayerNorm.bias', 'cls.predictions.decoder.bias', 'fnet.encoder.layer.0.output.LayerNorm.weight', 'fnet.encoder.layer.9.output.LayerNorm.weight', 'fnet.encoder.layer.4.intermediate.dense.weight', 'fnet.encoder.layer.0.output.dense.bias', 'fnet.encoder.layer.5.intermediate.dense.bias', 'fnet.encoder.layer.4.output.LayerNorm.weight', 'fnet.encoder.layer.10.intermediate.dense.weight', 'fnet.encoder.layer.1.output.LayerNorm.bias', 'fnet.encoder.layer.1.output.LayerNorm.weight', 'fnet.encoder.layer.7.output.dense.bias', 'fnet.encoder.layer.11.intermediate.dense.weight', 'fnet.encoder.layer.8.output.LayerNorm.bias', 'fnet.encoder.layer.0.fourier.output.LayerNorm.weight', 'fnet.encoder.layer.9.output.dense.weight', 'fnet.encoder.layer.10.output.LayerNorm.weight', 'fnet.encoder.layer.2.fourier.output.LayerNorm.bias', 'fnet.encoder.layer.2.output.LayerNorm.bias', 'fnet.encoder.layer.2.output.LayerNorm.weight', 'fnet.encoder.layer.8.output.LayerNorm.weight']\n",
            "- This IS expected if you are initializing FnetModule from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FnetModule from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FnetModule were not initialized from the model checkpoint at google/fnet-base and are newly initialized: ['encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.fourier.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.6.fourier.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.3.fourier.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.fourier.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.fourier.output.LayerNorm.weight', 'embeddings.projection.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.fourier.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.fourier.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.0.fourier.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.fourier.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.fourier.output.LayerNorm.weight', 'encoder.layer.5.fourier.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.2.fourier.output.LayerNorm.bias', 'encoder.layer.7.fourier.output.LayerNorm.weight', 'encoder.layer.4.fourier.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.fourier.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.fourier.output.LayerNorm.bias', 'encoder.layer.8.fourier.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.fourier.output.LayerNorm.bias', 'encoder.layer.10.fourier.output.LayerNorm.bias', 'encoder.layer.4.fourier.output.LayerNorm.weight', 'encoder.layer.6.fourier.output.LayerNorm.bias', 'encoder.layer.3.fourier.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.8.fourier.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'pooler.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'pooler.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.fourier.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'embeddings.projection.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "learning_rate = 1e-05\n",
        "bert_optimizer = torch.optim.Adam(params=bert_model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "bert_scheduler = torch.optim.lr_scheduler.StepLR(bert_optimizer, step_size=2, gamma=0.1)\n",
        "n_epochs=1"
      ],
      "metadata": {
        "id": "ke7e1kxBQou4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "learning_rate = 1e-05\n",
        "fnet_optimizer = torch.optim.Adam(params=fnet_model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "fnet_scheduler = torch.optim.lr_scheduler.StepLR(fnet_optimizer, step_size=2, gamma=0.1)\n",
        "n_epochs=1"
      ],
      "metadata": {
        "id": "Ua6iL-U5ji6k"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, train_dataloader, criterion, optimizer, device=\"cuda:0\"):\n",
        "    model.to(device).train()\n",
        "    with tqdm(total=len(train_dataloader)) as pbar:\n",
        "        for batch in train_dataloader:\n",
        "            # добавляем батч для вычисления на GPU\n",
        "            # Распаковываем данные из dataloader\n",
        "            input_ids1, attention_mask1, input_ids2, attention_mask2, labels = batch\n",
        "            input_ids1 = batch['input_ids1'].to(device)\n",
        "            attention_mask1 = batch['attention_mask1'].to(device)\n",
        "            input_ids2 = batch['input_ids2'].to(device)\n",
        "            attention_mask2 = batch['attention_mask2'].to(device)\n",
        "            # labels = torch.tensor(batch['labels'], dtype=torch.long).flatten().to(device)\n",
        "            labels = batch['labels'].view(-1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model.forward(x_input_ids=input_ids1, x_segment_ids=None, x_input_mask=attention_mask1,\n",
        "                                   y_input_ids=input_ids2, y_segment_ids=None, y_input_mask=attention_mask2, labels=labels)\n",
        "            \n",
        "            _, predicted = torch.max(output, 1)\n",
        "            \n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            _, predicted = torch.max(output.detach(), 1)\n",
        "            accuracy = accuracy_score(predicted.cpu().numpy(), labels.cpu().numpy())\n",
        "            pbar.set_description('Loss: {:.4f}; Accuracy: {:.4f}'.format(loss.item(), accuracy))    \n",
        "            pbar.update(1)\n",
        "\n",
        "def predict(model, val_dataloader, criterion, device=\"cuda:0\"):\n",
        "    model.to(device).eval()\n",
        "    losses = []\n",
        "    predicted_classes = []\n",
        "    true_classes = []\n",
        "    with tqdm(total=len(val_dataloader)) as pbar:\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dataloader:\n",
        "                \n",
        "                input_ids1, attention_mask1, input_ids2, attention_mask2, labels = batch\n",
        "                input_ids1 = batch['input_ids1'].to(device)\n",
        "                attention_mask1 = batch['attention_mask1'].to(device)\n",
        "                input_ids2 = batch['input_ids2'].to(device)\n",
        "                attention_mask2 = batch['attention_mask2'].to(device)\n",
        "                labels = batch['labels'].view(-1).to(device)\n",
        "                \n",
        "                \n",
        "                output = model.forward(x_input_ids=input_ids1, x_segment_ids=None, x_input_mask=attention_mask1,\n",
        "                                   y_input_ids=input_ids2, y_segment_ids=None, y_input_mask=attention_mask2, labels=labels)\n",
        "                _, predicted = torch.max(output, 1)\n",
        "            \n",
        "                loss = criterion(output, labels)\n",
        "                losses.append(loss.item())\n",
        "                _, predicted = torch.max(output.detach(), 1)\n",
        "                predicted_classes.append(predicted)\n",
        "                true_classes.append(labels)\n",
        "                \n",
        "                \n",
        "                accuracy_mae = accuracy_score(predicted.cpu().numpy(), labels.cpu().numpy())\n",
        "                pbar.set_description('Loss: {:.4f}; Accuracy_MAE: {:.4f}'.format(loss.item(), accuracy_mae))    \n",
        "                pbar.update(1)\n",
        "                \n",
        "    predicted_classes = torch.cat(predicted_classes).detach().to('cpu').numpy()\n",
        "    true_classes = torch.cat(true_classes).detach().to('cpu').numpy()\n",
        "    return losses, predicted_classes, true_classes\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader, criterion, optimizer, device=\"cuda:0\", n_epochs=10, scheduler=None):\n",
        "    model.to(device)\n",
        "    # lrs = []\n",
        "    for epoch in range(n_epochs):\n",
        "        print('Learning rate: ', optimizer.param_groups[0]['lr'])\n",
        "        print('Epoc:', epoch)\n",
        "        train_one_epoch(model, train_dataloader, criterion, optimizer)\n",
        "        print('Validation')\n",
        "        losses, predicted_classes, true_classes = predict(model, val_dataloader, criterion)\n",
        "        print('Accuracy_MAE: ', accuracy_score(true_classes, predicted_classes))\n",
        "        # lrs.append(optimizer.param_groups[0]['lr'])\n",
        "        scheduler.step()"
      ],
      "metadata": {
        "id": "njINDpSBkDds"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(fnet_model, fnet_train_dataloader, fnet_eval_dataloader, criterion, fnet_optimizer, device, n_epochs, fnet_scheduler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdpNdk0OkZtk",
        "outputId": "ecb0338c-f763-4684-962e-baa91b45a38d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate:  1e-05\n",
            "Epoc: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/141 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py:175: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  ../aten/src/ATen/native/Copy.cpp:239.)\n",
            "  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "Loss: 0.2900; Accuracy: 1.0000: 100%|██████████| 141/141 [00:39<00:00,  3.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.1976; Accuracy_MAE: 0.9286: 100%|██████████| 47/47 [00:04<00:00,  9.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy_MAE:  0.9346666666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(bert_model, bert_train_dataloader, bert_eval_dataloader, criterion, bert_optimizer, device, n_epochs, bert_scheduler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3Y44nUmQ1Ao",
        "outputId": "49912bd4-a29e-464a-d5ce-a1648b0f9371"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate:  1e-05\n",
            "Epoc: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.0152; Accuracy: 1.0000: 100%|██████████| 141/141 [00:56<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.0085; Accuracy_MAE: 1.0000: 100%|██████████| 47/47 [00:06<00:00,  7.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy_MAE:  0.988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "import numpy as np\n",
        "import faiss"
      ],
      "metadata": {
        "id": "AWGYs8HJzQqF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "# !pip install -U sentence-transformers\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lR3ycgYzQs-",
        "outputId": "58e9fb55-3586-4bce-e619-c8c91506f379"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.7/dist-packages (1.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "big_sentence = pd.read_csv('/content/drive/MyDrive/big_sentence.csv')\n",
        "sentences = big_sentence['sentence'].sample(1000).unique()"
      ],
      "metadata": {
        "id": "RqjFxcMF2S-Z"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens(tokenizer, sentences, max_length=64):\n",
        "  input_ids = []\n",
        "  attention_mask = []\n",
        "  token_type_ids = []\n",
        "\n",
        "  for sent in sentences:\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sent,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = max_length,           # Pad & truncate all sentences.\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      attention_mask.append(encoded_dict['attention_mask'])\n",
        "      token_type_ids.append(encoded_dict['token_type_ids'])\n",
        "\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_mask = torch.cat(attention_mask, dim=0)\n",
        "  token_type_ids = torch.cat(token_type_ids, dim=0)\n",
        "  return input_ids, attention_mask, token_type_ids\n",
        "\n",
        "\n",
        "def search_bert(index, model, tokenizer, query, max_length=128, sentences=None):\n",
        "   t=time.time()\n",
        "   encoded_dict = tokenizer.encode_plus(\n",
        "                        query,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = max_length,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "   model.eval()\n",
        "   with torch.no_grad():\n",
        "     output = model(x_input_ids=encoded_dict['input_ids'], x_input_mask=encoded_dict['attention_mask'], \n",
        "                    x_segment_ids=encoded_dict['token_type_ids'], train_sbert=False)\n",
        "\n",
        "\n",
        "   query_vector = output.cpu().numpy()\n",
        "   k = 5\n",
        "   top_k = index.search(query_vector, k)\n",
        "   print('totaltime: {}'.format(time.time()-t))\n",
        "   return [sentences[_id] for _id in top_k[1].tolist()[0]], query_vector\n",
        "\n",
        "def final_result(index=None, model=None, tokenizer=None, max_length=128, sentences=None):\n",
        "  query = str(input())\n",
        "  find_sentence, query_vector = search_bert(index=index, model=model, tokenizer=tokenizer, query=query, max_length=max_length, sentences=sentences)\n",
        "  input_ids, attention_mask, token_type_ids = get_tokens(tokenizer=tokenizer, sentences=find_sentence, max_length=64)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    sent_b = model(x_input_ids=input_ids, x_input_mask=attention_mask, x_segment_ids=token_type_ids, train_sbert=False)\n",
        "  cos_simil = cosine_similarity(query_vector, sent_b)[0]\n",
        "  print('results :')\n",
        "  for i in range(len(cos_simil)):\n",
        "    print('\\t','Cosine Similarity: ' + str(cos_simil[i]) + '  ' +str(find_sentence[i]))"
      ],
      "metadata": {
        "id": "PEYW8tNnU8qz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, attention_mask, token_type_ids = get_tokens(tokenizer=bert_tokenizer, sentences=sentences, max_length=64)\n",
        "input_ids, attention_mask, token_type_ids = input_ids.to(device), attention_mask.to(device), token_type_ids.to(device)\n",
        "bert_model.eval()\n",
        "with torch.no_grad():\n",
        "  output = bert_model(x_input_ids=input_ids, x_input_mask=attention_mask, x_segment_ids=token_type_ids, train_sbert=False)\n",
        "\n",
        "\n",
        "encoded_data = output.cpu().numpy()\n",
        "bert_index = faiss.IndexIDMap(faiss.IndexFlatIP(768))\n",
        "bert_index.add_with_ids(encoded_data, np.array(range(0, len(sentences))))\n",
        "faiss.write_index(bert_index, 'search')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS6zNVrm3sBP",
        "outputId": "bba2afee-d120-4dd2-df2a-dfcbe2bc9d7d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LYcfa3NRLtah"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_result(index=bert_index, model=bert_model.cpu(), tokenizer=bert_tokenizer, max_length=128, sentences=sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSHxlL82Lu3-",
        "outputId": "70545363-833f-4bb2-e85f-75a6fcd8c020"
      },
      "execution_count": 23,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cancer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "totaltime: 0.43050575256347656\n",
            "results :\n",
            "\t Cosine Similarity: 0.7884204  Recent clinical experience has provided evidence that conservative management and early prophylactic antibiotic administration in sterile necrotising pancreatitis is the treatment of choice [17,21,25,31].\n",
            "\t Cosine Similarity: 0.775641  Three studies examined the abilities of both devices to record objective data such as burn variables[35], daily food intake [40]and intravenous infusions of hemophilic clotting factor concentrates[32].\n",
            "\t Cosine Similarity: 0.78065777  Correction for gas compressibility as well as resistive and accelerative losses in the flexiVent, connecting tubing and the tracheal cannula were performed as described previously [38]using dynamic calibration data obtained by applying volume perturbations through the tubing and tracheal cannula first when it was completely closed and then when it was open to the atmosphere.\n",
            "\t Cosine Similarity: 0.77311754  Very recently, the application of gamma detection probe technology in radioguided surgery after a preoperative same-day injection of 18F-FDG has been reported in a limited number of selected cases of breast cancer [43,45,47,51].\n",
            "\t Cosine Similarity: 0.7731926  Peeker and Fall [4,5] defined a clear picture of ulcerative or classic IC that included urothelial spongiosis and detachment; subepithelial, perineural and perivascular deposits of mononuclear cells; and a characteristic mast cell response, with an increase of mast cells in detrusor muscle and in the lamina propria.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, attention_mask, token_type_ids = get_tokens(tokenizer=fnet_tokenizer, sentences=sentences, max_length=64)\n",
        "input_ids, attention_mask, token_type_ids = input_ids.to(device), attention_mask.to(device), token_type_ids.to(device)\n",
        "fnet_model.eval()\n",
        "with torch.no_grad():\n",
        "  output = fnet_model(x_input_ids=input_ids, x_input_mask=attention_mask, x_segment_ids=token_type_ids, train_sbert=False)\n",
        "\n",
        "\n",
        "encoded_data = output.cpu().numpy()\n",
        "fnet_index = faiss.IndexIDMap(faiss.IndexFlatIP(768))\n",
        "fnet_index.add_with_ids(encoded_data, np.array(range(0, len(sentences))))\n",
        "faiss.write_index(fnet_index, 'search')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORzhMjXhUVpI",
        "outputId": "3985ba7c-0163-4e7e-f3fe-a4f4ae31dd03"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_result(index=fnet_index, model=fnet_model.cpu(), tokenizer=fnet_tokenizer, max_length=128, sentences=sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyON9UDKUVmo",
        "outputId": "4cd50008-4fe2-4fd9-e370-480dae228be0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cancer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "totaltime: 0.3511521816253662\n",
            "results :\n",
            "\t Cosine Similarity: 0.6412494  The scientific literature has also investigated secondary prevention as a strategy to reduce disability from LBP because effective primary prevention strategies are currently lacking [11].\n",
            "\t Cosine Similarity: 0.63241494  The smallest number of participants to detect this difference between two proportions estimated from independent samples is 65 participants per group, ie, 130 participants in total [23].\n",
            "\t Cosine Similarity: 0.6219418  One of its particular mandated projects is a coordinated marketing and advertising strategy to attract medical graduates, including overseas-trained doctors, for employment in Queensland [16].\n",
            "\t Cosine Similarity: 0.63591504  In order to obtain functional profiles of the differentially regulated genes, the three gene clusters identified above were subjected to Gene Ontology analysis using the GOAT programme [33].\n",
            "\t Cosine Similarity: 0.63904876  Each volunteer was allowed to visit the bathroom, kitchen and for measurements of lung function reported elsewhere [34] and the median period outside the chamber was 99 minutes during 24-h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QbQb4Hj4UVfA"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}